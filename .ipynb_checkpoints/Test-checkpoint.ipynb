{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_wavelets import DWTForward, DWTInverse\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 32, 32])\n",
      "torch.Size([10, 5, 3, 32, 32])\n",
      "torch.Size([10, 5, 32, 32])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "xfm = DWTForward(J=1, wave='haar', mode='zero')\n",
    "X = torch.randn(10,5,64,64)\n",
    "N,C,H,W = \n",
    "Yl, Yh = xfm(X)\n",
    "print(Yl.shape)\n",
    "# >>> torch.Size([10, 5, 12, 12])\n",
    "print(Yh[0].shape)\n",
    "print(Yh[0][:,:,1,:,:].shape)\n",
    "print(len(Yh))\n",
    "# # >>> torch.Size([10, 5, 3, 34, 34])\n",
    "# print(Yh[1].shape)\n",
    "# # >>> torch.Size([10, 5, 3, 19, 19])\n",
    "# print(Yh[2].shape)\n",
    "# # >>> torch.Size([10, 5, 3, 12, 12])\n",
    "ifm = DWTInverse(wave='haar', mode='zero')\n",
    "Y = ifm((Yl, Yh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  torch.Size([10, 128, 64, 64])\n",
      "Yl shape:  torch.Size([10, 128, 32, 32])\n",
      "Yh shape:  torch.Size([10, 128, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjunarora/anaconda3/envs/EE367/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_cnn shape:  torch.Size([10, 512, 32, 32])\n",
      "Yh_cnn shape:  torch.Size([10, 128, 3, 32, 32])\n",
      "Yl_cnn shape:  torch.Size([10, 128, 32, 32])\n",
      "shape of Y:  torch.Size([10, 128, 64, 64])\n",
      "tensor(0.0002, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "xfm = DWTForward(J=1, wave='haar', mode='zero')\n",
    "X = torch.randn(10,128,64,64)\n",
    "print(\"X shape: \",X.shape)\n",
    "N,C,H,W = X.shape\n",
    "\n",
    "#ensure cnn is identity mapping to ensure we can test for differences\n",
    "cnn = nn.Conv2d(4*C,4*C,kernel_size=3,stride=1,padding=1,dilation=1,bias=True)\n",
    "torch.nn.init.constant(cnn.weight,0.0)\n",
    "# print(cnn.weight)\n",
    "\n",
    "Yl, Yh = xfm(X)\n",
    "Yh = Yh[0]\n",
    "print(\"Yl shape: \",Yl.shape)\n",
    "print(\"Yh shape: \",Yh.shape)\n",
    "\n",
    "\n",
    "#This code should go to WCNN \n",
    "#ensuring we can cat across this dimension\n",
    "Y_cnn = [Yh[:,:,0,:,:],Yh[:,:,1,:,:],Yh[:,:,2,:,:],Yl]\n",
    "Y_cnn = torch.cat(Y_cnn,dim=1)\n",
    "#DO CNN things here\n",
    "Y_cnn = cnn(Y_cnn)\n",
    "print(\"Y_cnn shape: \",Y_cnn.shape)\n",
    "\n",
    "#second layer here (ensures that we still get back correct layers after applying DWT a second time to concatenated Y_cnn)\n",
    "Yl_second,Yh_second = xfm(Y_cnn)\n",
    "\n",
    "Y_cnn = ifm((Yl_second,Yh_second))\n",
    "#end CNN things\n",
    "\n",
    "#ensure that we can unpack Y_cnn and correctly put dimensions where we mean to\n",
    "\n",
    "#this code should go to IWCNN\n",
    "Yh_cnn_list = torch.unsqueeze(Y_cnn[:,0:3*C,:,:],dim=2).view(N,C,3,int(H/2),int(W/2))\n",
    "# Yh_cnn_list = torch.unsqueeze(Y_cnn.view(N,4*C,3,H,W),dim=1)\n",
    "Yl_cnn = Y_cnn[:,3*C:,:,:]\n",
    "print(\"Yh_cnn shape: \",Yh_cnn_list.shape)\n",
    "print(\"Yl_cnn shape: \",Yl_cnn.shape)\n",
    "\n",
    "#put Yh_cnn back into list \n",
    "Yh_list = [Yh_cnn_list]\n",
    "ifm = DWTInverse(wave='haar', mode='zero')\n",
    "Y = ifm((Yl_cnn, Yh_list))\n",
    "print(\"shape of Y: \",Y.shape)\n",
    "\n",
    "#ensure the difference between DWT inverse and original image is negligible\n",
    "print(torch.mean(X-Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    multi-level wavelet CNN transform\n",
    "    param: in channels: number of input channels for each image (should be 1 for a single grayscale image or stack)\n",
    "    param: out channels: number of output channels (default: 4*C)\n",
    "    param: filter_sz: size of filter (should be 3)\n",
    "    param: num conv: number of conv-batch_norm-relu layers wanted beyond first\n",
    "    input:(N,in_ch,H,W)\n",
    "    output:(N,out_ch,H,W)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch=None,filter_sz=3,num_conv=3):\n",
    "        super(WCNN,self).__init__()\n",
    "        if out_ch is None:\n",
    "            out_ch = 4*in_ch\n",
    "        self.DwT= DWTForward(J=1, wave='haar', mode='zero')\n",
    "        #4 * input channels since DwT creates 4 outputs per image\n",
    "        modules = []\n",
    "        modules.append(nn.Conv2d(4*in_ch,out_ch,kernel_size=3,padding=1))\n",
    "        modules.append(nn.BatchNorm2d(num_features = out_ch))\n",
    "        modules.append(nn.ReLU())\n",
    "        for i in range(num_conv):\n",
    "            modules.append(nn.Conv2d(out_ch,out_ch,kernel_size=3,padding=1))\n",
    "            modules.append(nn.BatchNorm2d(num_features = out_ch))\n",
    "            modules.append(nn.ReLU())   \n",
    "        self.conv = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        Yl,Yh = self.DwT(x)\n",
    "        Yh = Yh[0]\n",
    "        Y_cnn = [Yh[:,:,0,:,:],Yh[:,:,1,:,:],Yh[:,:,2,:,:],Yl]\n",
    "        Y_cnn = torch.cat(Y_cnn,dim=1)\n",
    "        output = self.conv(Y_cnn)\n",
    "        return output\n",
    "    \n",
    "class IWCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    inverse of WCNN\n",
    "    param: in_ch: number of input channels for each image\n",
    "    param: out_ch: number of output channels for each internal cnn layer (ensure evenly divisble by 4)\n",
    "    param: filter_sz: size of filter (should be 3)\n",
    "    param: num_conv: number of conv-batch_norm-relu layers wanted beyond first\n",
    "    input: (N,in_ch,H,W)\n",
    "    output: (N,out_ch/4,H,W)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,in_ch,out_ch=None,filter_sz=3,num_conv=3):\n",
    "        super(IWCNN,self).__init__()\n",
    "        if out_ch is None: \n",
    "            out_ch = in_ch\n",
    "        self.IDwT = DWTInverse(wave='haar', mode='zero')\n",
    "        modules = []\n",
    "        modules.append(nn.Conv2d(in_ch,out_ch,kernel_size=3,padding=1))\n",
    "        modules.append(nn.BatchNorm2d(num_features = out_ch))\n",
    "        modules.append(nn.ReLU())\n",
    "        for i in range(num_conv):\n",
    "            modules.append(nn.Conv2d(out_ch,out_ch,kernel_size=3,padding=1))\n",
    "            modules.append(nn.BatchNorm2d(num_features = out_ch))\n",
    "            modules.append(nn.ReLU())   \n",
    "        self.conv = nn.Sequential(*modules)\n",
    "    \n",
    "        \n",
    "    def forward(self,x):\n",
    "        Y_cnn = self.conv(x)\n",
    "        N,C,H,W = Y_cnn.shape\n",
    "#         print(Y_cnn.shape)\n",
    "#         print(N,C,3,int(H),int(W))\n",
    "        Yh_cnn_list = torch.unsqueeze(Y_cnn[:,0:3*int(C/4),:,:],dim=2).view(N,int(C/4),3,int(H),int(W))\n",
    "        Yl_cnn = Y_cnn[:,3*int(C/4):,:,:]\n",
    "#         Yh_cnn_list = torch.unsqueeze(Y_cnn[:,0:3,:,:],dim=1)\n",
    "#         Yl_cnn = torch.unsqueeze(Y_cnn[:,-1,:,:],dim=1)\n",
    "#         print(Yh_cnn_list.shape)\n",
    "        Yh_list = [Yh_cnn_list]\n",
    "        Y = self.IDwT((Yl_cnn, Yh_list))\n",
    "        return Y \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing WCNN\n",
      "shape of X:  torch.Size([10, 128, 64, 64])\n",
      "shape of Y:  torch.Size([10, 2048, 16, 16])\n",
      "shape of output:  torch.Size([10, 128, 64, 64])\n",
      "tensor(-0.0950, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(\"testing WCNN\")\n",
    "X = torch.randn(10,128,64,64)\n",
    "N,C,H,W = X.shape\n",
    "cnn = WCNN(C,out_ch=4*C,num_conv=0) \n",
    "cnn_2 = WCNN(4*C,out_ch=16*C)\n",
    "inv_cnn = IWCNN(in_ch=16*C,out_ch= 16*C,num_conv=0)\n",
    "inv_cnn_2 = IWCNN(in_ch=4*C,out_ch = 4*C,num_conv=0)\n",
    "Y = cnn(X) # N,4*C,H,W\n",
    "Y = cnn_2(Y) #N,16*C,H,W\n",
    "\n",
    "output = inv_cnn(Y) #N,4*C,H,W\n",
    "output = inv_cnn_2(output) #N,C,H,W\n",
    "print(\"shape of X: \",X.shape)\n",
    "print(\"shape of Y: \",Y.shape)\n",
    "print(\"shape of output: \",output.shape)\n",
    "print(torch.mean(X-output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EE367)",
   "language": "python",
   "name": "ee367"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
