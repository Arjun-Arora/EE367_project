{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./src/models/\")\n",
    "sys.path.append(\"./src/\")\n",
    "import utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "from MWU_CNN import MW_Unet\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    print('Image device and mean')\n",
    "    print(img.device)\n",
    "    print(img.mean())\n",
    "    output_image = img.cpu().numpy().transpose((1,2,0))\n",
    "    npimg = np.interp(output_image,(-1.0,1.0),(0,255.0)).astype(np.uint8)\n",
    "    print('Mean of image: {}'.format(npimg.mean()))\n",
    "    #format H,W,C\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "def save_image(img, path):\n",
    "#     img = img * 0.5 + 0.5\n",
    "    torchvision.utils.save_image(img, path)\n",
    "    \n",
    "def backprop(optimizer,model_output,target):\n",
    "    optimizer.zero_grad()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(model_output,target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "def get_PSNR(model_output,target):\n",
    "    I_hat = model_output.cpu().detach().numpy()\n",
    "    I = target.cpu().detach().numpy()\n",
    "    mse = (np.square(I-I_hat)).mean(axis=None)\n",
    "    PSNR = 10 * np.log10(1.0/mse)\n",
    "    return PSNR\n",
    "def train(args):\n",
    "    \"\"\"\n",
    "    train model\n",
    "    \"\"\"\n",
    "    \n",
    "    ####################################### Initializing Model #######################################\n",
    "    \n",
    "    step = 0.01\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print_every = int(args['--print_every'])\n",
    "    num_epochs = int(args['--num_epochs'])\n",
    "    save_every = int(args['--save_every'])\n",
    "    save_path = str(args['--model_save_path'])\n",
    "    batch_size = int(args['--batch_size'])\n",
    "    train_data_path = args['--data_path']\n",
    "    n  = int(args['--n'])\n",
    "    train_split,val_split = args['--train_split'],args['--val_split']\n",
    "    \n",
    "    img_directory = args['--train_img_directory']\n",
    "\n",
    "    model = MW_Unet(num_conv=3,in_ch=1)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=step)\n",
    "    \n",
    "    ######################################### Loading Data ##########################################\n",
    "    dataset_total = utils.patchesDataset(patches_path=train_data_path,n=n)\n",
    "    \n",
    "    train_max_idx = int(train_split*len(dataset_total))\n",
    "#     print(train_max_idx)\n",
    "    dataset_train = torch.utils.data.Subset(dataset_total,range(0,train_max_idx))\n",
    "    val_max_idx = train_max_idx+int(val_split*len(dataset_total))\n",
    "    dataset_val = torch.utils.data.Subset(dataset_total,range(train_max_idx,val_max_idx))\n",
    "#     print(test_max_idx)\n",
    "    \n",
    "#     dataset_test = torch.utils.data.Subset(dataset_total,range(val_max_idx,test_max_idx))\n",
    "    \n",
    "    dataloader_train = DataLoader(dataset_train,batch_size=batch_size)\n",
    "    dataloader_val = DataLoader(dataset_val,batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    print(\"length of train set: \",len(dataset_train))\n",
    "    print(\"length of val set: \",len(dataset_val))\n",
    "#     print(\"length of test set: \",len(dataset_test))\n",
    "    \n",
    "    train_PSNRs = []\n",
    "    train_losses = []\n",
    "    val_PSNRs = []\n",
    "    val_losses = []\n",
    "    init_epoch = 0 \n",
    "    \n",
    "    best_val_PSNR = 0.0\n",
    "    try:\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "                #INITIATE dataloader_train\n",
    "                print(\"epoch: \",epoch)\n",
    "                with tqdm(total = len(dataloader_train)) as pbar:\n",
    "                    for index, sample in enumerate(dataloader_train):\n",
    "\n",
    "                        model.train()\n",
    "\n",
    "                        target,model_input = sample['target'],sample['input']\n",
    "                        target = target.to(device)\n",
    "                        model_input = model_input.to(device)\n",
    "\n",
    "                        output = model.forward(model_input)\n",
    "\n",
    "                        train_loss = backprop(optimizer,output,target)\n",
    "\n",
    "                        train_PSNR = get_PSNR(output,target)\n",
    "\n",
    "                        avg_val_PSNR = []\n",
    "                        avg_val_loss = []\n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            for val_index,sample in enumerate(dataloader_val):\n",
    "                                target,model_input = sample['target'],sample['input']\n",
    "                                \n",
    "                                target = target.to(device)\n",
    "                                model_input = model_input.to(device)\n",
    "                                \n",
    "                                output = model.forward(model_input)\n",
    "                                loss_fn = nn.MSELoss()\n",
    "                                loss_val = loss_fn(output,target)\n",
    "                                PSNR = get_PSNR(output,target)\n",
    "                                avg_val_PSNR.append(PSNR)\n",
    "                                avg_val_loss.append(loss_val.cpu().detach().numpy())\n",
    "                        avg_val_PSNR = np.mean(avg_val_PSNR)\n",
    "                        avg_val_loss = np.mean(avg_val_loss)\n",
    "                        val_PSNRs.append(avg_val_PSNR)\n",
    "                        val_losses.append(avg_val_loss)\n",
    "\n",
    "                        train_losses.append(train_loss.cpu().detach().numpy())\n",
    "                        train_PSNRs.append(train_PSNR)\n",
    "\n",
    "                        if index == len(dataloader_train) - 1:\n",
    "                            img_grid = output.data\n",
    "                            img_grid = torchvision.utils.make_grid(img_grid)\n",
    "                            directory = img_directory\n",
    "                            save_image(img_grid,'{}train_img_{}.png'.format(directory,epoch))\n",
    "                            print('train images')\n",
    "                            imshow(img_grid)\n",
    "                            \n",
    "                        pbar.update(1)\n",
    "                    if epoch % print_every == 0: \n",
    "                        print (\"Epoch: {}, Loss: {}, Training PSNR: {}\".format(epoch, train_loss, train_PSNR))\n",
    "                        print (\"Epoch: {}, Avg Val Loss: {},Avg Val PSNR: {}\".format(epoch, avg_val_loss, avg_val_PSNR))\n",
    "                    if epoch % save_every == 0 and best_val_PSNR < avg_val_PSNR:\n",
    "                        best_val_PSNR = avg_val_PSNR\n",
    "                        print(\"new best Avg Val PSNR: {}\".format(best_val_PSNR))\n",
    "                        print (\"Saving model to {}\".format(save_path))\n",
    "                        torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(), \n",
    "                            'optimizer_state_dict': optimizer.state_dict(), \n",
    "                            'loss': train_loss}, \n",
    "                             save_path)\n",
    "                        print (\"Saved successfully to {}\".format(save_path))\n",
    "                        \n",
    "                        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interupted...\")\n",
    "        print (\"Saving model to {}\".format(save_path))\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict(), \n",
    "                    'loss': train_loss}, \n",
    "                     save_path)\n",
    "        print (\"Saved successfully to {}\".format(save_path))          \n",
    "        \n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    \n",
    "    return (train_losses,train_PSNRs,val_losses,val_PSNRs,best_val_PSNR)\n",
    "\n",
    "def Test(args):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print_every = int(args['--print_every'])\n",
    "    num_epochs = int(args['--num_epochs'])\n",
    "    save_every = int(args['--save_every'])\n",
    "    batch_size = int(args['--batch_size'])\n",
    "    n  = int(args['--n'])\n",
    "    train_split,val_split,test_split = args['--train_split'],args['--val_split'],args['--test_split']\n",
    "    data_path = args['--data_path']\n",
    "    model_path = args['--model_save_path']\n",
    "    img_directory = args['--test_img_directory']\n",
    "    ################################ Load Data ###################################################\n",
    "    dataset_total = utils.patchesDataset(patches_path=data_path,n=n)\n",
    "    train_max_idx = int(train_split*len(dataset_total))\n",
    "    val_max_idx = train_max_idx+int(val_split*len(dataset_total))\n",
    "    test_max_idx = val_max_idx + int(test_split*len(dataset_total))\n",
    "    dataset_test = torch.utils.data.Subset(dataset_total,range(val_max_idx,test_max_idx))\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size = batch_size)\n",
    "    \n",
    "#     print(len(dataset_test))\n",
    "    load_path = model_path\n",
    "    \n",
    "    model = MW_Unet(num_conv=3,in_ch=1)\n",
    "    model.to(device)\n",
    "    \n",
    "    if(load_path != None):\n",
    "        print(\"Loading model from {}\".format(load_path))\n",
    "        checkpoint = torch.load(load_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        #epoch = checkpoint['epoch']\n",
    "        print(\"Model successfully loaded from {}\".format(load_path))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "    \n",
    "    test_loss = []\n",
    "    test_PSNR = []\n",
    "    \n",
    "    with tqdm(total=len(dataloader_test)) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for index, sample in enumerate(dataloader_test):\n",
    "                target,model_input = sample['target'],sample['input']\n",
    "                target = target.to(device)\n",
    "                model_input = model_input.to(device)\n",
    "\n",
    "                output = model.forward(model_input)\n",
    "                \n",
    "                loss_fn = nn.MSELoss()\n",
    "\n",
    "                loss = loss_fn(output,target)\n",
    "                PSNR = get_PSNR(output,target)\n",
    "                \n",
    "                test_loss.append(loss.cpu().numpy())\n",
    "                test_PSNR.append(PSNR)\n",
    "                \n",
    "                if index == len(dataloader_test) - 1:\n",
    "                    img_grid = output.data\n",
    "                    img_grid = torchvision.utils.make_grid(img_grid)\n",
    "                    directory = img_directory\n",
    "                    save_image(img_grid,'{}test_img.png'.format(directory))\n",
    "                    print('test images')\n",
    "                    imshow(img_grid)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                \n",
    "    test_loss,test_PSNR = np.mean(np.array(test_loss)), np.mean(np.array(test_PSNR))\n",
    "    \n",
    "    str_to_save = \"Test_loss: \" + str(test_loss) + \" , Test PSNR: \" + str(test_PSNR)\n",
    "    \n",
    "    with open(\"test_results.txt\",'a') as test_writer:\n",
    "        test_writer.write(str_to_save + \"\\n\")\n",
    "    \n",
    "    return (test_loss,test_PSNR)\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "                    \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel_1: 16, channel_2: 32\n",
      "loading patches from patches directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 683.27it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed loading patches from directory!\n",
      "(2000, 240, 240)\n",
      "shape of target: (2000, 1, 240, 240) shape of noisy: (2000, 1, 240, 240)\n",
      "length of train set:  1600\n",
      "length of val set:  200\n",
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [00:18<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images\n",
      "Image device and mean\n",
      "cuda:0\n",
      "tensor(138.6729, device='cuda:0')\n",
      "Mean of image: 253.36613713647668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.16it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best Avg Val PSNR: 14.940141660485905\n",
      "Saving model to ./experiments/baseline/baseline.pt\n",
      "Saved successfully to ./experiments/baseline/baseline.pt\n",
      "epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [00:18<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images\n",
      "Image device and mean\n",
      "cuda:0\n",
      "tensor(126.8211, device='cuda:0')\n",
      "Mean of image: 253.36613713647668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.24it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best Avg Val PSNR: 18.355592971843734\n",
      "Saving model to ./experiments/baseline/baseline.pt\n",
      "Saved successfully to ./experiments/baseline/baseline.pt\n",
      "epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [00:18<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images\n",
      "Image device and mean\n",
      "cuda:0\n",
      "tensor(123.3186, device='cuda:0')\n",
      "Mean of image: 253.36613713647668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.24it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best Avg Val PSNR: 20.51637476745227\n",
      "Saving model to ./experiments/baseline/baseline.pt\n",
      "Saved successfully to ./experiments/baseline/baseline.pt\n",
      "epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:09<00:09,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interupted...\n",
      "Saving model to ./experiments/baseline/baseline.pt\n",
      "Saved successfully to ./experiments/baseline/baseline.pt\n",
      "Training completed.\n",
      "Final training loss: 0.009517055936157703, Best Validation PSNR: 20.51637476745227\n",
      "loading patches from patches directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:03<00:00, 616.45it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed loading patches from directory!\n",
      "(2000, 240, 240)\n",
      "shape of target: (2000, 1, 240, 240) shape of noisy: (2000, 1, 240, 240)\n",
      "channel_1: 16, channel_2: 32\n",
      "Loading model from ./experiments/baseline/baseline.pt\n",
      "Model successfully loaded from ./experiments/baseline/baseline.pt\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:00<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test images\n",
      "Image device and mean\n",
      "cuda:0\n",
      "tensor(134.8218, device='cuda:0')\n",
      "Mean of image: 253.36613713647668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 4/4 [00:00<00:00,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.011239456944167614, Test PSNR: 19.499345130184764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = {'--print_every':5,\n",
    "        '--num_epochs':10,\n",
    "        '--save_every':1,\n",
    "        '--data_path':'./data/patches/',\n",
    "        '--batch_size':64,\n",
    "        '--model_save_path': \"./experiments/baseline/baseline.pt\",\n",
    "        '--n': 2000,\n",
    "        '--test_split': 0.1,\n",
    "        '--val_split':0.1,\n",
    "        '--train_split':0.8,\n",
    "        '--train_img_directory':'./experiments/images/train_images/',\n",
    "        '--test_img_directory': './experiments/images/test_images/'\n",
    "        }\n",
    "train_losses,train_PSNRs,val_losses,val_PSNRs,best_val_PSNR = train(args)\n",
    "np.save('train_PSNRs.npy', np.array(train_PSNRs))\n",
    "np.save('train_losses.npy', np.array(train_losses))\n",
    "np.save('val_PSNRs.npy', np.array(val_PSNRs))\n",
    "np.save('val_losses.npy', np.array(val_losses))\n",
    "\n",
    "loss = train_losses[-1]\n",
    "print(\"Final training loss: {}, Best Validation PSNR: {}\".format(loss, best_val_PSNR))\n",
    "\n",
    "test_loss,test_PSNR = Test(args)\n",
    "print(\"Test loss: {}, Test PSNR: {}\".format(test_loss, test_PSNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python EE367",
   "language": "python",
   "name": "ee367"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
